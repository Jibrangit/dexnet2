{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/jibran/dexnet_ws/gqcnn/data/training/dex-net_2.0/tensors\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "DATA = os.getenv('DEXNET_DATA')\n",
    "print(DATA)                     #Confirm if path is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_keys(['table_mask', 'depth_ims_raw', 'image_labels', 'hand_poses', 'depth_ims_tf_table', 'robust_ferrari_canny', 'binary_ims_tf', 'binary_ims_raw', 'force_closure', 'object_labels', 'depth_ims_raw_table', 'pose_labels', 'depth_ims_tf'])\n"
     ]
    }
   ],
   "source": [
    "arrays = {}\n",
    "datapoint = '_05590.npz'                #Enter datapoint here (anywhere between _00000 and _06728, there are a total of 6.7 million datapoints i.e, 6728 x 1000). Change this value to just '.npz' to train on entire dataset\n",
    "for filename in os.listdir(DATA):\n",
    "    if filename.endswith(datapoint):\n",
    "        arrays[filename.replace(datapoint, '')] = np.load('/home/jibran/dexnet_ws/gqcnn/data/training/dex-net_2.0/tensors/'+filename)\n",
    "\n",
    "features = {}\n",
    "for array in arrays:\n",
    "    f = arrays[array]\n",
    "    feature = f['arr_0.npy']\n",
    "    features[array] = feature\n",
    "print(features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1000,)\n(1000,)\n(1000, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "#Inputs to feed into CNN\n",
    "aligned_imgs = features['depth_ims_tf_table']\n",
    "# print(aligned_imgs.shape)\n",
    "gripper_depths = features['hand_poses'][:, 2]          #aligned_imgs and gripper_depths are the \"X\" of our model (Refer to gqcnn/Data/README.md for more info)\n",
    "# print(gripper_depths.shape)\n",
    "grasp_metrics = features['robust_ferrari_canny']       #Y of our model (Grasp metric)\n",
    "\n",
    "#Verify shapes \n",
    "\n",
    "print(grasp_metrics.shape)\n",
    "print(gripper_depths.shape)\n",
    "print(aligned_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "\n",
    "# example of defining the discriminator model\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras import layers\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Lambda\n",
    "from tensorflow.nn import local_response_normalization\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "# from tensorflow.keras import layers\n",
    "from keras.layers import Dense, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<KerasTensor: shape=(None, 1024) dtype=float32 (created by layer 'dense')>,\n",
       " <KerasTensor: shape=(None, 32, 32, 1) dtype=float32 (created by layer 'img')>)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "#TODO: Initialize weights in layers according to the paper https://arxiv.org/pdf/1703.09312.pdf\n",
    "def getGraspQualityVariable():\n",
    "    input = Input(shape=(32, 32, 1), name=\"img\")\n",
    "    x1 = Conv2D(filters=64, kernel_size=7, activation='relu')(input)\n",
    "    x2 = Conv2D(filters=64, kernel_size=5, activation='relu')(x1)\n",
    "    x3 = Lambda(local_response_normalization)(x2)\n",
    "    x4 = MaxPooling2D(pool_size=(2, 2), strides=2)(x3)\n",
    "    x5 = Conv2D(filters=64, kernel_size=3, activation='relu')(x4)\n",
    "    x6 = Conv2D(filters=64, kernel_size=3, activation='relu')(x5)\n",
    "    x7 = Lambda(local_response_normalization)(x6)\n",
    "    x8 = Flatten()(x7)\n",
    "    x9 = Dense(1024, activation='relu')(x8)\n",
    "    \n",
    "    # plot_model(model, to_file='GraspQualityModel_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    return x9, input\n",
    "    \n",
    "getGraspQualityVariable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPointcloudModel():\n",
    "    input = Input(shape=(1), name=\"z\")\n",
    "    x = Dense(16, input_dim=1, activation='relu')(input)\n",
    "    return x, input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.merge import concatenate\n",
    "def getDexnet2Model():\n",
    "    grasp_model, input_1 = getGraspQualityVariable()\n",
    "    pc_model, input_2 = getPointcloudModel()\n",
    "\n",
    "    out = Dense(1024, activation='relu')\n",
    "\n",
    "    # x = layers.concatenate([grasp_model, pc_model])\n",
    "    merge = concatenate([grasp_model, pc_model])\n",
    "    out_1 = Dense(1024, activation='relu')(merge)\n",
    "    out_2 = Dense(2, activation='softmax')(out_1)\n",
    "    model = Model(inputs=[input_1, input_2], outputs=out_2)\n",
    "    plot_model(model, to_file='Dexnet2_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    return model\n",
    "\n",
    "first_model = getDexnet2Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_2\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nimg (InputLayer)                [(None, 32, 32, 1)]  0                                            \n__________________________________________________________________________________________________\nconv2d_12 (Conv2D)              (None, 26, 26, 64)   3200        img[0][0]                        \n__________________________________________________________________________________________________\nconv2d_13 (Conv2D)              (None, 22, 22, 64)   102464      conv2d_12[0][0]                  \n__________________________________________________________________________________________________\nlambda_6 (Lambda)               (None, 22, 22, 64)   0           conv2d_13[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_3 (MaxPooling2D)  (None, 11, 11, 64)   0           lambda_6[0][0]                   \n__________________________________________________________________________________________________\nconv2d_14 (Conv2D)              (None, 9, 9, 64)     36928       max_pooling2d_3[0][0]            \n__________________________________________________________________________________________________\nconv2d_15 (Conv2D)              (None, 7, 7, 64)     36928       conv2d_14[0][0]                  \n__________________________________________________________________________________________________\nlambda_7 (Lambda)               (None, 7, 7, 64)     0           conv2d_15[0][0]                  \n__________________________________________________________________________________________________\nflatten_3 (Flatten)             (None, 3136)         0           lambda_7[0][0]                   \n__________________________________________________________________________________________________\nz (InputLayer)                  [(None, 1)]          0                                            \n__________________________________________________________________________________________________\ndense_11 (Dense)                (None, 1024)         3212288     flatten_3[0][0]                  \n__________________________________________________________________________________________________\ndense_12 (Dense)                (None, 16)           32          z[0][0]                          \n__________________________________________________________________________________________________\nconcatenate_2 (Concatenate)     (None, 1040)         0           dense_11[0][0]                   \n                                                                 dense_12[0][0]                   \n__________________________________________________________________________________________________\ndense_14 (Dense)                (None, 1024)         1065984     concatenate_2[0][0]              \n__________________________________________________________________________________________________\ndense_15 (Dense)                (None, 2)            2050        dense_14[0][0]                   \n==================================================================================================\nTotal params: 4,459,874\nTrainable params: 4,459,874\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "first_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "25/25 [==============================] - 5s 160ms/step - loss: 0.1218 - accuracy: 0.6123 - val_loss: 0.0000e+00 - val_accuracy: 0.4650\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/first_model.weights.best.hdf5\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1fe85f3580>"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights/first_model.weights.best.hdf5', verbose = 1, save_best_only=True)\n",
    "x_train = [aligned_imgs, gripper_depths]                                   #Check model.summary() in previous section and check model architecture in paper\n",
    "y_train = grasp_metrics                                                    #valued between [0, 1] grasp robustness for the given grasp\n",
    "first_model.fit(x_train,\n",
    "          y_train,\n",
    "          batch_size=32,                                                   #worth ecperimenting\n",
    "          epochs=1,                                      \n",
    "          validation_split=0.2,                                            #Decide on a number\n",
    "          callbacks=[checkpointer])                                        #CHange the filepath of the checkpointer varianble to store different version of weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.         0.         0.         0.         0.00113762 0.\n 0.00576939 0.         0.         0.         0.         0.\n 0.         0.0058793  0.         0.00123383 0.         0.00164662\n 0.         0.         0.         0.         0.         0.00113762\n 0.         0.00576939 0.         0.         0.         0.\n 0.         0.         0.0058793  0.         0.00123383 0.\n 0.00164662 0.         0.         0.         0.         0.\n 0.00113762 0.         0.00576939 0.         0.         0.\n 0.         0.         0.         0.0058793  0.         0.00123383\n 0.         0.00164662 0.         0.         0.         0.\n 0.         0.00113762 0.         0.00576939 0.         0.\n 0.         0.         0.         0.         0.0058793  0.\n 0.00123383 0.         0.00164662 0.         0.         0.\n 0.         0.         0.00113762 0.         0.00576939 0.\n 0.         0.         0.         0.         0.         0.0058793\n 0.         0.00123383 0.         0.00164662 0.         0.\n 0.         0.         0.         0.00113762 0.         0.00576939\n 0.         0.         0.         0.         0.         0.\n 0.0058793  0.         0.00123383 0.         0.00164662 0.\n 0.         0.         0.         0.         0.00113762 0.\n 0.00576939 0.         0.         0.         0.         0.\n 0.         0.0058793  0.         0.00123383 0.         0.00164662\n 0.         0.         0.         0.         0.         0.00113762\n 0.         0.00576939 0.         0.         0.         0.\n 0.         0.         0.0058793  0.         0.00123383 0.\n 0.00164662 0.         0.         0.         0.         0.\n 0.00113762 0.         0.00576939 0.         0.         0.\n 0.         0.         0.         0.0058793  0.         0.00123383\n 0.         0.00164662 0.         0.         0.         0.\n 0.         0.00113762 0.         0.00576939 0.         0.\n 0.         0.         0.         0.         0.0058793  0.\n 0.00123383 0.         0.00164662 0.         0.         0.\n 0.         0.         0.00113762 0.         0.00576939 0.\n 0.         0.         0.         0.         0.         0.0058793\n 0.         0.00123383 0.         0.00164662 0.         0.\n 0.         0.         0.         0.00113762 0.         0.00576939\n 0.         0.         0.         0.         0.         0.\n 0.0058793  0.         0.00123383 0.         0.00164662 0.\n 0.         0.         0.         0.         0.00113762 0.\n 0.00576939 0.         0.         0.         0.         0.\n 0.         0.0058793  0.         0.00123383 0.         0.00164662\n 0.         0.         0.         0.         0.         0.00113762\n 0.         0.00576939 0.         0.         0.         0.\n 0.         0.         0.0058793  0.         0.00123383 0.\n 0.00164662 0.         0.         0.         0.         0.\n 0.00113762 0.         0.00576939 0.         0.         0.\n 0.         0.         0.         0.0058793  0.         0.00123383\n 0.         0.00164662 0.         0.         0.         0.\n 0.         0.00113762 0.         0.00576939 0.         0.\n 0.         0.         0.         0.         0.0058793  0.\n 0.00123383 0.         0.00164662 0.         0.         0.\n 0.         0.         0.00113762 0.         0.00576939 0.\n 0.         0.         0.         0.         0.         0.0058793\n 0.         0.00123383 0.         0.00164662 0.         0.\n 0.         0.         0.         0.00113762 0.         0.00576939\n 0.         0.         0.         0.         0.         0.\n 0.0058793  0.         0.00123383 0.         0.00164662 0.\n 0.         0.         0.         0.         0.00113762 0.\n 0.00576939 0.         0.         0.         0.         0.\n 0.         0.0058793  0.         0.00123383 0.         0.00164662\n 0.         0.         0.         0.         0.         0.00113762\n 0.         0.00576939 0.         0.         0.         0.\n 0.         0.         0.0058793  0.         0.00123383 0.\n 0.00164662 0.         0.         0.         0.         0.\n 0.00113762 0.         0.00576939 0.         0.         0.\n 0.         0.         0.         0.0058793  0.         0.00123383\n 0.         0.00164662 0.         0.         0.         0.\n 0.         0.00113762 0.         0.00576939 0.         0.\n 0.         0.         0.         0.         0.0058793  0.\n 0.00123383 0.         0.00164662 0.         0.         0.\n 0.         0.         0.00113762 0.         0.00576939 0.\n 0.         0.         0.         0.         0.         0.0058793\n 0.         0.00123383 0.         0.00164662 0.         0.\n 0.         0.         0.         0.00113762 0.         0.00576939\n 0.         0.         0.         0.         0.         0.\n 0.0058793  0.         0.00123383 0.         0.00164662 0.\n 0.         0.         0.         0.         0.00113762 0.\n 0.00576939 0.         0.         0.         0.         0.\n 0.         0.0058793  0.         0.00123383 0.         0.00164662\n 0.         0.         0.         0.         0.         0.00113762\n 0.         0.00576939 0.         0.         0.         0.\n 0.         0.         0.0058793  0.         0.00123383 0.\n 0.00164662 0.         0.         0.         0.         0.\n 0.00113762 0.         0.00576939 0.         0.         0.\n 0.         0.         0.         0.0058793  0.         0.00123383\n 0.         0.00164662 0.         0.         0.         0.\n 0.         0.00113762 0.         0.00576939 0.         0.\n 0.         0.         0.         0.         0.0058793  0.\n 0.00123383 0.         0.00164662 0.         0.         0.\n 0.         0.         0.00113762 0.         0.00576939 0.\n 0.         0.         0.         0.         0.         0.0058793\n 0.         0.00123383 0.         0.00164662 0.         0.\n 0.         0.         0.         0.00113762 0.         0.00576939\n 0.         0.         0.         0.         0.         0.\n 0.0058793  0.         0.00123383 0.         0.00164662 0.\n 0.         0.         0.         0.         0.00113762 0.\n 0.00576939 0.         0.         0.         0.         0.\n 0.         0.0058793  0.         0.00123383 0.         0.00164662\n 0.         0.         0.         0.         0.         0.00113762\n 0.         0.00576939 0.         0.         0.         0.\n 0.         0.         0.0058793  0.         0.00123383 0.\n 0.00164662 0.         0.         0.         0.         0.\n 0.00113762 0.         0.00576939 0.         0.         0.\n 0.         0.         0.         0.0058793  0.         0.00123383\n 0.         0.00164662 0.         0.         0.         0.\n 0.         0.00113762 0.         0.00576939 0.         0.\n 0.         0.         0.         0.         0.0058793  0.\n 0.00123383 0.         0.00164662 0.         0.         0.\n 0.         0.         0.00113762 0.         0.00576939 0.\n 0.         0.         0.         0.         0.         0.0058793\n 0.         0.00123383 0.         0.00164662 0.         0.\n 0.         0.         0.         0.00113762 0.         0.00576939\n 0.         0.         0.         0.         0.         0.\n 0.0058793  0.         0.00123383 0.         0.00164662 0.\n 0.         0.         0.         0.         0.00113762 0.\n 0.00576939 0.         0.         0.         0.         0.\n 0.         0.0058793  0.         0.00123383 0.         0.00164662\n 0.         0.         0.         0.         0.         0.00113762\n 0.         0.00576939 0.         0.         0.         0.\n 0.         0.         0.0058793  0.         0.00123383 0.\n 0.00164662 0.         0.         0.         0.         0.\n 0.00113762 0.         0.00576939 0.         0.         0.\n 0.         0.         0.         0.0058793  0.         0.00123383\n 0.         0.00164662 0.         0.         0.         0.\n 0.         0.00113762 0.         0.00576939 0.         0.\n 0.         0.         0.         0.         0.0058793  0.\n 0.00123383 0.         0.00164662 0.         0.         0.\n 0.         0.         0.00113762 0.         0.00576939 0.\n 0.         0.         0.         0.         0.         0.0058793\n 0.         0.00123383 0.         0.00164662 0.         0.\n 0.         0.         0.         0.00113762 0.         0.00576939\n 0.         0.         0.         0.         0.         0.\n 0.0058793  0.         0.00123383 0.         0.00164662 0.\n 0.         0.         0.         0.         0.00258259 0.\n 0.0061406  0.01127583 0.00343862 0.00212299 0.00679051 0.00380425\n 0.         0.00486693 0.         0.         0.         0.0058793\n 0.         0.         0.00229818 0.         0.         0.00258259\n 0.         0.0061406  0.01127583 0.00343862 0.00212299 0.00679051\n 0.00380425 0.         0.00486693 0.         0.         0.\n 0.0058793  0.         0.         0.00229818 0.         0.\n 0.00258259 0.         0.0061406  0.01127583 0.00343862 0.00212299\n 0.00679051 0.00380425 0.         0.00486693 0.         0.\n 0.         0.0058793  0.         0.         0.00229818 0.\n 0.         0.00258259 0.         0.0061406  0.01127583 0.00343862\n 0.00212299 0.00679051 0.00380425 0.         0.00486693 0.\n 0.         0.         0.0058793  0.         0.         0.00229818\n 0.         0.         0.00258259 0.         0.0061406  0.01127583\n 0.00343862 0.00212299 0.00679051 0.00380425 0.         0.00486693\n 0.         0.         0.         0.0058793  0.         0.\n 0.00229818 0.         0.         0.00258259 0.         0.0061406\n 0.01127583 0.00343862 0.00212299 0.00679051 0.00380425 0.\n 0.00486693 0.         0.         0.         0.0058793  0.\n 0.         0.00229818 0.         0.         0.00258259 0.\n 0.0061406  0.01127583 0.00343862 0.00212299 0.00679051 0.00380425\n 0.         0.00486693 0.         0.         0.         0.0058793\n 0.         0.         0.00229818 0.         0.         0.00258259\n 0.         0.0061406  0.01127583 0.00343862 0.00212299 0.00679051\n 0.00380425 0.         0.00486693 0.         0.         0.\n 0.0058793  0.         0.         0.00229818 0.         0.\n 0.00258259 0.         0.0061406  0.01127583 0.00343862 0.00212299\n 0.00679051 0.00380425 0.         0.00486693 0.         0.\n 0.         0.0058793  0.         0.         0.00229818 0.\n 0.         0.00258259 0.         0.0061406  0.01127583 0.00343862\n 0.00212299 0.00679051 0.00380425 0.         0.00486693 0.\n 0.         0.         0.0058793  0.         0.         0.00229818\n 0.         0.         0.00258259 0.         0.0061406  0.01127583\n 0.00343862 0.00212299 0.00679051 0.00380425]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}